{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gridsearch_ANN_4Fitur_dengan_Fitur_KVP_dengan_Feature_Scaling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3vLuXOe-DamF",
        "-Akr2y4T7RHL",
        "Y69ZCgMv1Q2H",
        "vTAoJWcCWf5A",
        "BP7eIiQM71RM"
      ],
      "authorship_tag": "ABX9TyMc5yhV/4drqJqODgEAnKle",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozanryo/air-kerma-hvl-prediction-AI/blob/main/Gridsearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayrIPCg0nwxk"
      },
      "source": [
        "# **Mounting Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErfOA89bnxMy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5153b85d-6aba-4061-a0f4-c25e3b8c81f6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vLuXOe-DamF"
      },
      "source": [
        "# **Melakukan Install Pydicom**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSeKaF2CDaQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c33ee9-d10b-4bcc-e8f2-237b95deda0c"
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/56/342e1f8ce5afe63bf65c23d0b2c1cd5a05600caad1c211c39725d3a4cc56/pydicom-2.0.0-py3-none-any.whl (35.4MB)\n",
            "\u001b[K     |████████████████████████████████| 35.5MB 1.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Akr2y4T7RHL"
      },
      "source": [
        "# **Import Library**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMJvB6fk7QPA"
      },
      "source": [
        "import pydicom as dicom\n",
        "import os\n",
        "\n",
        "#Preprocessing Dataset dan Membangun Neural Network\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "\n",
        "#Pengukuran Performa Machine Learning\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "from sklearn import metrics\n",
        "from keras.utils import plot_model\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y69ZCgMv1Q2H"
      },
      "source": [
        "# **Membuat Fungsi**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_1pNcIc1Q2N"
      },
      "source": [
        "**Membuat Fungsi Input dan Output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQMpUgQf1Q2N"
      },
      "source": [
        "def Input(Data):\n",
        "    X = Data.iloc[:, 0:4].values\n",
        "    labelencoder_X_1 = LabelEncoder()\n",
        "    \n",
        "    X[:, 3] = labelencoder_X_1.fit_transform(X[:, 3])\n",
        "    ct = ColumnTransformer([('one_hot_encoder', OneHotEncoder(categories='auto'), [3])], \n",
        "                               remainder='passthrough')\n",
        "    X = ct.fit_transform(X)\n",
        "    \n",
        "    #Standard Scaler()\n",
        "    sc = StandardScaler()\n",
        "    X[:, 3:6] = sc.fit_transform(X[:, 3:6])\n",
        "    X[:, 3:6] = sc.transform(X[:, 3:6])\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTzITIY-1Q2S"
      },
      "source": [
        "def Labels(Data):\n",
        "    y = Data.iloc[:, 4:7].values\n",
        "    \n",
        "    labelencoder_y_1 = LabelEncoder()\n",
        "    y[:,0] = labelencoder_y_1.fit_transform(y[:,0])\n",
        "    labelencoder_y_3 = LabelEncoder()\n",
        "    y[:,1] = labelencoder_y_3.fit_transform(y[:,1])\n",
        "    \n",
        "    ct1 = ColumnTransformer([('one_hot_encoder', OneHotEncoder(categories='auto'), [0, 1])], \n",
        "                               remainder='passthrough')\n",
        "    \n",
        "    y = ct1.fit_transform(y) \n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOmwplOI1Q2U"
      },
      "source": [
        "**Membuat Fungsi Pembuat Model ANN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttukQjgV1Q2U"
      },
      "source": [
        "def neural_net(n_layers, inputs, units, activation, output_activation, optimizer):\n",
        "    if isinstance(units, list):\n",
        "        assert len(units) == n_layers\n",
        "    else:\n",
        "        units = [units] * n_layers\n",
        "        \n",
        "    classifier = Sequential()\n",
        " \n",
        "    # Adds first hidden layer with input_dim parameter\n",
        "    classifier.add(Dense(units = units[0],\n",
        "                         input_dim = inputs,\n",
        "                         activation = activation,\n",
        "                         kernel_initializer = 'glorot_uniform',\n",
        "                         name = 'h1'))\n",
        "    \n",
        "    # Adds remaining hidden layers\n",
        "    for i in range(2, n_layers + 1):\n",
        "        classifier.add(Dense(units = units[i-1], \n",
        "                        activation = activation, \n",
        "                        kernel_initializer = 'glorot_uniform', \n",
        "                        name = 'h{}'.format(i)))\n",
        "    \n",
        "    # Adds output layer\n",
        "    classifier.add(Dense(units = y_test_folds.shape[1], \n",
        "                         activation = output_activation, \n",
        "                         kernel_initializer='glorot_uniform', name='o'))\n",
        " \n",
        "    # Compiles the model\n",
        "    classifier.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', 'mean_squared_error'])\n",
        " \n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3oBsxt5ajRW"
      },
      "source": [
        "**Membuat Model Umum untuk dilakukan GridSearch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60ECUo-2aJQV"
      },
      "source": [
        "def create_model(n_layers=1, units=10, activation='sigmoid', output_activation='sigmoid', optimizer='Adam'):\n",
        "    if isinstance(units, list):\n",
        "        assert len(units) == n_layers\n",
        "    else:\n",
        "        units = [units] * n_layers\n",
        "        \n",
        "    classifier = Sequential()\n",
        " \n",
        "    # Adds first hidden layer with input_dim parameter\n",
        "    classifier.add(Dense(units = units[0],\n",
        "                         input_dim = 3,\n",
        "                         activation = activation,\n",
        "                         kernel_initializer = 'glorot_uniform',\n",
        "                         name = 'h1'))\n",
        "    \n",
        "    # Adds remaining hidden layers\n",
        "    for i in range(2, n_layers + 1):\n",
        "        classifier.add(Dense(units = units[i-1], \n",
        "                        activation = activation, \n",
        "                        kernel_initializer = 'glorot_uniform', \n",
        "                        name = 'h{}'.format(i)))\n",
        "    \n",
        "    # Adds output layer\n",
        "    classifier.add(Dense(units = 6, \n",
        "                         activation = output_activation, \n",
        "                         kernel_initializer='glorot_uniform', name='o'))\n",
        " \n",
        "    # Compiles the model\n",
        "    classifier.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', 'mean_squared_error'])\n",
        " \n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcNRDhNi1Q2X"
      },
      "source": [
        "**Membuat Fungsi Plotting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_aUT61w1Q2Y"
      },
      "source": [
        "def plot_result(model):\n",
        "    # Plot Loss dari Model\n",
        "    plot_loss = plt.plot(model.history['loss'], label='Train')\n",
        "    plot_loss = plt.plot(model.history['val_loss'], label='Validation')\n",
        "    plot_loss = plt.title('Mean Absolute Error')\n",
        "    plot_loss = plt.grid(True)\n",
        "    plot_loss = plt.ylabel('Loss Value')\n",
        "    plot_loss = plt.xlabel('No. epoch')\n",
        "    plot_loss = plt.legend(loc=\"upper right\")\n",
        "    plot_loss = plt.show()\n",
        "\n",
        "    # Plot Accuracy dari Model\n",
        "    plot_acc = plt.plot(model.history['accuracy'], label='Train')\n",
        "    plot_acc = plt.plot(model.history['val_accuracy'], label='Validation')\n",
        "    plot_acc = plt.title('Accuracy')\n",
        "    plot_acc = plt.grid(True)\n",
        "    plot_acc = plt.ylabel('Accuracy Value')\n",
        "    plot_acc = plt.xlabel('No. epoch')\n",
        "    plot_acc = plt.legend(loc=\"upper right\")\n",
        "    plot_acc = plt.show()\n",
        "\n",
        "    # Plot Mean Squared Error dari Model\n",
        "    plot_MSE = plt.plot(model.history['mean_squared_error'], label='MSE Train')\n",
        "    plot_MSE = plt.plot(model.history['val_mean_squared_error'], label='MSE Validation')\n",
        "    plot_MSE = plt.title('Mean Squared Error')\n",
        "    plot_MSE = plt.grid(True)\n",
        "    plot_MSE = plt.ylabel('MSE value')\n",
        "    plot_MSE = plt.xlabel('No. epoch')\n",
        "    plot_MSE = plt.legend(loc=\"upper left\")\n",
        "    plot_MSE = plt.show()\n",
        "    \n",
        "    return plot_loss, plot_acc, plot_MSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX_l6FfD1Q2a"
      },
      "source": [
        "**Membuat Fungsi Pengukur Performa Machine Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxZybCba1Q2a"
      },
      "source": [
        "def Precision_Classification_Kerma(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    ## Prediction on Kerma\n",
        "    y_pred_Kerma = y_pred[:, 0:3]\n",
        "    y_pred_Kerma = np.argmax(y_pred_Kerma, axis=1)\n",
        "     \n",
        "    ## Take a test on Kerma\n",
        "    y_test_Kerma = y_test[:, 0:3]\n",
        "    y_test_Kerma = np.argmax(y_test_Kerma, axis=1)\n",
        "    \n",
        "    Precision = precision_score(y_test_Kerma, y_pred_Kerma, average='micro')\n",
        "    return Precision\n",
        "\n",
        "def Precision_Classification_HVL(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "     \n",
        "    ## Prediction on HVL\n",
        "    y_pred_HVL = y_pred[:, 3:6]\n",
        "    y_pred_HVL = np.argmax(y_pred_HVL, axis=1)\n",
        "     \n",
        "    ## Take a test on HVL\n",
        "    y_test_HVL = y_test[:, 3:6]\n",
        "    y_test_HVL = np.argmax(y_test_HVL, axis=1)\n",
        "    \n",
        "    Precision = precision_score(y_test_HVL, y_pred_HVL, average='micro')\n",
        "    return Precision\n",
        "\n",
        "def Recall_Classification_Kerma(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    ## Prediction on Kerma\n",
        "    y_pred_Kerma = y_pred[:, 0:3]\n",
        "    y_pred_Kerma = np.argmax(y_pred_Kerma, axis=1)\n",
        "     \n",
        "    ## Take a test on Kerma\n",
        "    y_test_Kerma = y_test[:, 0:3]\n",
        "    y_test_Kerma = np.argmax(y_test_Kerma, axis=1)\n",
        "    \n",
        "    Recall = recall_score(y_test_Kerma, y_pred_Kerma, average='micro')\n",
        "    return Recall\n",
        "\n",
        "def Recall_Classification_HVL(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "     \n",
        "    ## Prediction on HVL\n",
        "    y_pred_HVL = y_pred[:, 3:6]\n",
        "    y_pred_HVL = np.argmax(y_pred_HVL, axis=1)\n",
        "     \n",
        "    ## Take a test on HVL\n",
        "    y_test_HVL = y_test[:, 3:6]\n",
        "    y_test_HVL = np.argmax(y_test_HVL, axis=1)\n",
        "    \n",
        "    Recall = recall_score(y_test_HVL, y_pred_HVL, average='micro')\n",
        "    return Recall\n",
        "\n",
        "def f1Score_Classification_Kerma(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    ## Prediction on Kerma\n",
        "    y_pred_Kerma = y_pred[:, 0:3]\n",
        "    y_pred_Kerma = np.argmax(y_pred_Kerma, axis=1)\n",
        "     \n",
        "    ## Take a test on Kerma\n",
        "    y_test_Kerma = y_test[:, 0:3]\n",
        "    y_test_Kerma = np.argmax(y_test_Kerma, axis=1)\n",
        "    \n",
        "    f1 = f1_score(y_test_Kerma, y_pred_Kerma, average='micro')\n",
        "    return f1\n",
        "\n",
        "def f1Score_Classification_HVL(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "     \n",
        "    ## Prediction on HVL\n",
        "    y_pred_HVL = y_pred[:, 3:6]\n",
        "    y_pred_HVL = np.argmax(y_pred_HVL, axis=1)\n",
        "     \n",
        "    ## Take a test on HVL\n",
        "    y_test_HVL = y_test[:, 3:6]\n",
        "    y_test_HVL = np.argmax(y_test_HVL, axis=1)\n",
        "    \n",
        "    f1 = f1_score(y_test_HVL, y_pred_HVL, average='micro')\n",
        "    return f1\n",
        "\n",
        "def Accuracy_Classification_Kerma(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    ## Prediction on Kerma\n",
        "    y_pred_Kerma = y_pred[:, 0:3]\n",
        "    y_pred_Kerma = np.argmax(y_pred_Kerma, axis=1)\n",
        "     \n",
        "    ## Take a test on Kerma\n",
        "    y_test_Kerma = y_test[:, 0:3]\n",
        "    y_test_Kerma = np.argmax(y_test_Kerma, axis=1)\n",
        "    \n",
        "    acc = accuracy_score(y_test_Kerma, y_pred_Kerma)\n",
        "    return acc\n",
        "\n",
        "def Accuracy_Classification_HVL(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "     \n",
        "    ## Prediction on HVL\n",
        "    y_pred_HVL = y_pred[:, 3:6]\n",
        "    y_pred_HVL = np.argmax(y_pred_HVL, axis=1)\n",
        "     \n",
        "    ## Take a test on HVL\n",
        "    y_test_HVL = y_test[:, 3:6]\n",
        "    y_test_HVL = np.argmax(y_test_HVL, axis=1)\n",
        "    \n",
        "    acc = accuracy_score(y_test_HVL, y_pred_HVL)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT_pF8rv2PJC"
      },
      "source": [
        "**Plot Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-keAp3zJpK4"
      },
      "source": [
        "def Conf_Mat(X_test_folds, y_test_folds):\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn.metrics import ConfusionMatrixDisplay\n",
        "  from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "  y_pred = NN.predict(X_test_folds)\n",
        "    \n",
        "  ## Prediction on Kerma\n",
        "  y_pred_Kerma = y_pred[:, 0:3]\n",
        "  y_pred_Kerma = np.argmax(y_pred_Kerma, axis=1)\n",
        "     \n",
        "  ## Take a test on Kerma\n",
        "  y_test_Kerma = y_test_folds[:, 0:3]\n",
        "  y_test_Kerma = np.argmax(y_test_Kerma, axis=1)\n",
        "\n",
        "  ## Prediction on HVL\n",
        "  y_pred_HVL = y_pred[:, 3:6]\n",
        "  y_pred_HVL = np.argmax(y_pred_HVL, axis=1)\n",
        "     \n",
        "  ## Take a test on HVL\n",
        "  y_test_HVL = y_test_folds[:, 3:6]\n",
        "  y_test_HVL = np.argmax(y_test_HVL, axis=1)\n",
        "\n",
        "  \n",
        "\n",
        "  titles_options = [(\"Confusion Matrix on Air Kerma Predictions\", y_pred_Kerma, y_test_Kerma, 'Air Kerma A', 'Air Kerma B', 'Air Kerma C'),\n",
        "                    (\"Confusion Matrix on Half Value Layer Predictions\", y_pred_HVL, y_test_HVL, 'HVL A', 'HVL B', 'HVL C')]\n",
        "      \n",
        "  for title, pred_y, test_y, A, B, C in titles_options:\n",
        "      conf_mat = metrics.confusion_matrix(pred_y, test_y)\n",
        "      disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat,\n",
        "                                    display_labels=[(A), (B), (C)])\n",
        "      \n",
        "      disp = disp.plot(include_values=True,\n",
        "                       cmap=plt.cm.Blues)\n",
        "      \n",
        "      disp.ax_.set_title(title)\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HltWPyQ1Q2c"
      },
      "source": [
        "**Melakukan Prediksi Data Image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4r-6Kp11Q2d"
      },
      "source": [
        "def Prediction_Kerma(Prediction_result):\n",
        "  if Prediction_result == 0:\n",
        "    print('The Prediction Kerma is on A')\n",
        "  elif Prediction_result == 1:\n",
        "    print('The Prediction Kerma is on B')\n",
        "  elif Prediction_result == 2:\n",
        "    print('The Prediction Kerma is on C')\n",
        "\n",
        "def Prediction_HVL(Prediction_result):\n",
        "  if Prediction_result == 0:\n",
        "    print('The Prediction HVL is on A')\n",
        "  elif Prediction_result == 1:\n",
        "    print('The Prediction HVL is on B')\n",
        "  elif Prediction_result == 2:\n",
        "    print('The Prediction HVL is on C')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNKXMMv7z2Q2"
      },
      "source": [
        "def False_Prediction_Kerma(False_1, False_2, real_kVp, False_kVp_1, False_kVp_2):\n",
        "  if False_1 == np.argmax(real_kVp):\n",
        "    print('False Prediction on real Kerma detected')\n",
        "  elif False_1 == np.argmax(False_kVp_1):\n",
        "    print('Prediction on another Kerma is detected')\n",
        "  else :\n",
        "    print('Kerma Prediction is accurate')\n",
        "  \n",
        "  if False_2 == np.argmax(real_kVp):\n",
        "    print('False Prediction on real Kerma detected')\n",
        "  elif False_2 == np.argmax(False_kVp_2):\n",
        "    print('Prediction on another Kerma is detected')\n",
        "  else :\n",
        "    print('Kerma Prediction is accurate')\n",
        "\n",
        "def False_Prediction_HVL(False_1, False_2, real_kVp, False_kVp_1, False_kVp_2):\n",
        "  if False_1 == np.argmax(real_kVp):\n",
        "    print('False Prediction on real HVL detected')\n",
        "  elif False_1 == np.argmax(False_kVp_1):\n",
        "    print('Prediction on another HVL is detected')\n",
        "  else :\n",
        "    print('HVL Prediction is accurate')\n",
        "  \n",
        "  if False_2 == np.argmax(real_kVp):\n",
        "    print('False Prediction on real HVL detected')\n",
        "  elif False_2 == np.argmax(False_kVp_2):\n",
        "    print('Prediction on another HVL is detected')\n",
        "  else :\n",
        "    print('HVL Prediction is accurate')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfs_wQ3X1Q2h"
      },
      "source": [
        "**Membuat fungsi untuk Pengambilan Gambar dengan Koordinat utama pada tengah Image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWJODIMd1Q2h"
      },
      "source": [
        "def crop_center(img, cropx, cropy):\n",
        "  y,x = img.shape\n",
        "  startx = x//2-(cropx//2)\n",
        "  starty = y//2-(cropy//2)    \n",
        "  return img[starty:starty+cropy,startx:startx+cropx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC7mp-c5GhYU"
      },
      "source": [
        "**Menggandakan Array**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZdT3LmeGhzB"
      },
      "source": [
        "def multiply_array(arr, n):\n",
        "    result = np.zeros(shape=(n*len(arr), 1))\n",
        "    idx = 0\n",
        "    for _ in range(n):\n",
        "      for element in arr:\n",
        "        result[idx, 0] = int(element)\n",
        "        idx += 1\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W5WkBHoGiGa"
      },
      "source": [
        "**Menggandakan Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjDxWbCoGiev"
      },
      "source": [
        "def multiply_data_kVp(arr, n):\n",
        "  count = n\n",
        "\n",
        "  a = np.asarray(arr[:, 0])\n",
        "  b = np.asarray(arr[:, 1])\n",
        "  c = np.asarray(arr[:, 2])\n",
        "\n",
        "  a = multiply_array(a, count)\n",
        "  b = multiply_array(b, count)\n",
        "  c = multiply_array(c, count)\n",
        "\n",
        "  result = np.append(a, b, axis=1)\n",
        "  result = np.append(result, c, axis=1)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTAoJWcCWf5A"
      },
      "source": [
        "# **Membuat Fungsi Pengujian Model dengan Citra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1msH5KJ41Q2p"
      },
      "source": [
        "**Membuat Fungsi Pembaca Dicom pada Suatu Direktori**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSypeQID1Q2q"
      },
      "source": [
        "def Read_Dicom(PathDicom):\n",
        "  lstFileDicom = [] #Create an Empty List\n",
        "  for dirName, subdirlist, filelist in os.walk(PathDicom):\n",
        "    for filename in filelist:\n",
        "      if '.dcm' in filename.lower():\n",
        "        lstFileDicom.append(os.path.join(dirName, filename))\n",
        "  \n",
        "  # Get ref file\n",
        "  RefDs = dicom.read_file(lstFileDicom[0])\n",
        "  \n",
        "  # Load dimensions based on the number of rows, columns, and slices (along the Z axis)\n",
        "  ConstPixelDims = (int(RefDs.Rows), int(RefDs.Columns), len(lstFileDicom))\n",
        "  \n",
        "  # Load spacing values (in mm)\n",
        "  ConstPixelSpacing = (float(RefDs.PixelSpacing[0]), float(RefDs.PixelSpacing[1]), float(RefDs.SliceThickness))\n",
        "  \n",
        "  # The array is sized based on 'ConstPixelDims'\n",
        "  ArrayDicom = np.zeros(ConstPixelDims, dtype=RefDs.pixel_array.dtype)\n",
        "  \n",
        "  # loop through all the DICOM files\n",
        "  for filenameDCM in lstFileDicom:\n",
        "      # read the file\n",
        "      ds = dicom.read_file(filenameDCM)\n",
        "      \n",
        "      # store the raw image data\n",
        "      ArrayDicom[:, :,lstFileDicom.index(filenameDCM)] = ds.pixel_array\n",
        "\n",
        "  return ArrayDicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zmWybj51Q2s"
      },
      "source": [
        "**Melakukan Pemeriksaan GLCM pada suatu Kumpulan File Dicom**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ooa5JKo1Q2s"
      },
      "source": [
        "def Check_GLCM(Dicom_file):\n",
        "  dissimilar = []\n",
        "  energi = []\n",
        "  contrast = []\n",
        "\n",
        "  dcm_files = Dicom_file.shape[2]\n",
        "\n",
        "  for i in range(dcm_files):\n",
        "    img = Dicom_file[:, :, i]\n",
        "    img = crop_center(img, 10, 10)\n",
        "\n",
        "    glcm = greycomatrix(img, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], 2048, symmetric=True, normed=True)\n",
        "\n",
        "    dissimilar.append(greycoprops(glcm, 'dissimilarity')[0, 0])\n",
        "    energi.append(greycoprops(glcm, 'energy')[0, 0])\n",
        "    contrast.append(greycoprops(glcm, 'contrast')[0, 0])\n",
        "\n",
        "  diss = np.asarray(dissimilar)\n",
        "  en = np.asarray(energi)\n",
        "  con = np.asarray(contrast)\n",
        "\n",
        "  Dataset = [diss, energi, con]\n",
        "  Dataset = np.asarray(Dataset)\n",
        "  Dataset = np.transpose(Dataset)\n",
        "\n",
        "  return Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvL9fKnb1Q2u"
      },
      "source": [
        "**Membuat Fungsi Prediksi pada Direktori 80 kVp**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMOg4V7y1Q2v"
      },
      "source": [
        "def Predict_Group(pred_material, Dicom_file, PathDicom):\n",
        "\n",
        "  dcm_files = Dicom_file.shape[2]\n",
        "  sc = StandardScaler()\n",
        "  pred_material = sc.fit_transform(pred_material)\n",
        "  pred_material = sc.transform(pred_material)\n",
        "\n",
        "  y_pred_group = NN.predict(pred_material)\n",
        "\n",
        "  lstFileDicom = [] #Create an Empty List\n",
        "  for dirName, subdirlist, filelist in os.walk(PathDicom):\n",
        "    for filename in filelist:\n",
        "      if '.dcm' in filename.lower():\n",
        "        lstFileDicom.append(os.path.join(dirName, filename))\n",
        "\n",
        "  for j in range(dcm_files):\n",
        "    y_pred_kerma_grup = np.argmax(y_pred_group[j, 0:3])\n",
        "    y_pred_HVL_grup = np.argmax(y_pred_group[j, 3:6])\n",
        "\n",
        "    A = np.asarray(lstFileDicom)\n",
        "\n",
        "    print('-------------------------------------------------------------------------')\n",
        "    print('File :', A[j])\n",
        "    Prediction_Kerma(y_pred_kerma_grup)\n",
        "    Prediction_HVL(y_pred_HVL_grup)\n",
        "    print('-------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP7eIiQM71RM"
      },
      "source": [
        "# **Membuka Dataset untuk diolah**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM1Y5kIf8zAf"
      },
      "source": [
        "path = '/content/drive/My Drive/Data/Dataset/New Data/Dataset_Update_4fitur.xlsx'\n",
        "\n",
        "dataset = pd.read_excel(path, sheet_name='Main Shuffle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW3BGSbx86t_"
      },
      "source": [
        "X = Input(dataset)\n",
        "y = Labels(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuncqRqIXOfY"
      },
      "source": [
        "# **Melakukan Pencarian Hyperparameter terbaik menggunakan GridsearchCV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okvJr4v8XV69"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kk69eqVhh7r"
      },
      "source": [
        "def create_model(n_layers=1, units=10, activation='sigmoid', output_activation='sigmoid', optimizer='Adam', init_mode='glorot_uniform'):\n",
        "    if isinstance(units, list):\n",
        "        assert len(units) == n_layers\n",
        "    else:\n",
        "        units = [units] * n_layers\n",
        "        \n",
        "    classifier = Sequential()\n",
        " \n",
        "    # Adds first hidden layer with input_dim parameter\n",
        "    classifier.add(Dense(units = units[0],\n",
        "                         input_dim = 6,\n",
        "                         activation = activation,\n",
        "                         kernel_initializer = init_mode,\n",
        "                         name = 'h1'))\n",
        "    \n",
        "    # Adds remaining hidden layers\n",
        "    for i in range(2, n_layers + 1):\n",
        "        classifier.add(Dense(units = units[i-1], \n",
        "                        activation = activation, \n",
        "                        kernel_initializer = init_mode, \n",
        "                        name = 'h{}'.format(i)))\n",
        "    \n",
        "    # Adds output layer\n",
        "    classifier.add(Dense(units = 6, \n",
        "                         activation = output_activation, \n",
        "                         kernel_initializer=init_mode, name='o'))\n",
        " \n",
        "    # Compiles the model\n",
        "    classifier.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', 'mean_squared_error'])\n",
        " \n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyzW8sOzdmAK"
      },
      "source": [
        "**Mencari Layer terbaik**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ6YRWoT8gzP"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "layers = [6, 7, 8]\n",
        "epoch = [100]\n",
        "optimizer = ['Adam']\n",
        "activation = ['sigmoid', 'elu']\n",
        "output_activation = ['softmax']\n",
        "units = [75, 100]\n",
        "kernel_init = ['glorot_uniform']\n",
        "\n",
        "param_grid = dict(n_layers=layers,\n",
        "                  units=units, \n",
        "                  epochs=epoch, \n",
        "                  activation=activation,\n",
        "                  output_activation=output_activation,\n",
        "                  optimizer=optimizer, \n",
        "                  init_mode=kernel_init)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
        "grid_result = grid.fit(X, y)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}